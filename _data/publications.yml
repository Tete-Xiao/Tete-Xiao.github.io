- title: 'Unified Perceptual Parsing for Scene Understanding'
  date:  'Mar. 2018'
  imgurl: '/images/projects/2018/tete2018_upp.png'
  imgprop: 'frame'
  selected: true
  has_equal_contribution: true
  authors:
    - name: <strong>T. Xiao</strong>
      url:  'http://tetexiao.com'
      equal_contribution: true
    - name: Y. Liu
      url:  ''
      equal_contribution: true
    - name: B. Zhou
      url:  'https://people.csail.mit.edu/bzhou'
      equal_contribution: true
    - name: Y. Jiang
      url:  'https://yuningjiang.github.io/'
    - name: J. Sun
      url:  'http://www.jiansun.org/'
  publisher:  'ECCV2018'
  place:    'in München, Germany'
  desc: 'Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identi- fying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recog- nize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes.'
  tags:
    - name: 'arXiv'
      url:  'https://arxiv.org/abs/1807.10221'
    - name: 'pdf'
      url:  '/pdf/tete2018_upp.pdf'
    - name: 'code'
      url:  'https://github.com/CSAILVision/unifiedparsing'


- title: 'Acquisition of Localization Confidence for Accurate Object Detection'
  date:  'Mar. 2018'
  imgurl: '/images/projects/2018/tete2018_iounet.png'
  imgprop: 'frame'
  selected: true
  has_equal_contribution: true
  authors:
    - name: R. Luo
      url:  ''
      equal_contribution: true
    - name: B. Jiang
      url:  ''
      equal_contribution: true
    - name: J. Mao
      url:  'http://vccy.xyz/'
    - name: <strong>T. Xiao</strong>
      url:  'http://tetexiao.com'
    - name: Y. Jiang
      url:  'https://yuningjiang.github.io/'
    - name: J. Sun
      url:  'http://www.jiansun.org/'
  publisher: 'ECCV2018'
  place:    'in München, Germany'
  status:   '(Oral)'
  desc: 'Modern CNN-based object detectors rely on bounding box regression and non-maximum suppression (NMS) to localize objects. While the probabilities for class labels naturally reflect classification confidence, localization confidence is absent. This makes properly localized bounding boxes degenerate during iterative regression or even suppressed during NMS. We propose IoU-Net learning to predict the IoU between each detected bounding box and the matched ground-truth. The network acquires this confidence of localization, which improves NMS procedure by preserving accurately localized bounding boxes. Furthermore, formulating the predicted IoU as an optimization objective provides monotonic improvement in bounding box localization. Extensive experiments on the MS-COCO dataset show the advance of IoU-Net and its compatibility and adaptivity to several state-of-the-art object detectors.'
  tags:
    - name: 'arXiv'
      url:  'https://arxiv.org/abs/1807.11590'
    - name: 'code'
      url:  'https://github.com/vacancy/PreciseRoIPooling'


- title: 'Learning Visually-Grounded Semantics from Contrastive Adversarial Samples'
  date:  'Mar. 2018'
  imgurl: '/images/projects/2018/tete2018_vge.png'
  imgprop: 'frame'
  selected: true
  has_equal_contribution: true
  authors:
    - name: H. Shi
      url:  'https://explorerfreda.github.io/'
      equal_contribution: true
    - name: J. Mao
      url:  'http://vccy.xyz/'
      equal_contribution: true
    - name: <strong>T. Xiao</strong>
      url:  'http://tetexiao.com'
      equal_contribution: true
    - name: Y. Jiang
      url:  'https://yuningjiang.github.io'
    - name: J. Sun
      url:  'http://www.jiansun.org/'
  publisher:  'COLING 2018'
  place:    'in Santa Fe, New Mexico'
  desc: 'We study the problem of grounding distributional representations of texts on the visual domain, namely visual-semantic embeddings (VSE for short). Begin with an insightful adversarial attack on VSE embeddings, we show the limitation of current frameworks and image-text datasets (e.g., MS-COCO). We show that the model is restricted to establish the link between textual semantics and visual concepts. We alleviate this problem by augmenting the MS-COCO image captioning datasets with synthesized textual contrastive adversarial samples. The samples enforce the model to ground learned embeddings to concrete concepts within the image. This simple but powerful technique brings a noticeable improvement over the baselines on a diverse set of downstream tasks, in addition to defending known-type adversarial attacks.'
  tags:
    - name: 'arXiv'
      url:  'https://arxiv.org/abs/1806.10348'
    - name: 'pdf'
      url:  '/pdf/tete2018_vse.pdf'
    - name: 'code'
      url:  'https://github.com/ExplorerFreda/VSE-C'


- title: 'MegDet: A Large Mini-Batch Object Detector'
  date:  'Nov. 2017'
  imgurl: '/images/projects/2018/tete2018_megdet.png'
  imgprop: 'frame'
  selected: true
  has_equal_contribution: true
  authors:
    - name: C. Peng
      url:  ''
      equal_contribution: true
    - name: <strong>T. Xiao</strong>
      url:  'http://tetexiao.com'
      equal_contribution: true
    - name: Z. Li
      url:  ''
      equal_contribution: true
    - name: Y. Jiang
      url:  'https://yuningjiang.github.io/'
    - name: X. Zhang
      url:  'https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=zh-CN'
    - name: K. Jia
      url:  ''
    - name: G. Yu
      url:  'https://sites.google.com/site/skicyyu/'
    - name: J. Sun
      url:  'http://www.jiansun.org/'
  publisher:  'CVPR 2018'
  place: 'in Salt Lake City, Utah'
  status:   '(Spotlight)'
  desc: 'Mini-batch size, a key factor in the training, has not been well studied in previous works on object detection. In this paper, we propose a Large Mini-Batch Object Detector (MegDet) to enable the training with much larger mini-batch size than previous work (i.e., from 16 to 256), so that we are able to effectively utilize multiple GPUs (up to 128 in our experiments) to significantly accelerate training procedure. Our detector is trained in much shorter time yet achieves better accuracy. The MegDet is the backbone of our submission (mmAP 52.5%) to COCO 2017 Challenge, where we won the 1st place of Detection task.'
  tags:
    - name: 'arXiv'
      url:  'https://arxiv.org/abs/1711.07240'
    - name: 'pdf'
      url:  '/pdf/tete2018_megdet.pdf'

- title: 'Repulsion Loss: Detecting Pedestrians in a Crowd'
  date:  'Nov. 2017'
  imgurl: '/images/projects/2018/tete2018_repulsionloss.png'
  imgprop: 'frame'
  selected: true
  has_equal_contribution: no
  authors:
    - name: X. Wang
      url:  ''
    - name: <strong>T. Xiao</strong>
      url:  'http://tetexiao.com'
    - name: Y. Jiang
      url:  'https://yuningjiang.github.io/'
    - name: S. Shao
      url:  ''
    - name: J. Sun
      url:  'http://www.jiansun.org/'    
    - name: C. Shen
      url:  'https://cs.adelaide.edu.au/~chhshen/'
  publisher:  'CVPR 2018'
  place: 'in Salt Lake City, Utah'
  desc: 'Detecting individual pedestrians in a crowd remains a challenging problem. In this paper, we explore how a state-of-the-art pedestrian detector is harmed by crowd occlusion and propose a novel crowd-robust regression loss specifically designed for crowd scenes. This loss is driven by two motivations: the attraction by target, and the repulsion by other surrounding objects. Our detector trained by repulsion loss outperforms all the state-of-the-art methods with a significant improvement in occlusion cases.'
  tags:
    - name: 'arXiv'
      url:  'https://arxiv.org/abs/1711.07752'
    - name: 'pdf'
      url:  '/pdf/tete2018_repulsionloss.pdf'

- title: 'What Can Help Pedestrian Detection?'
  date:  'July. 2017'
  imgurl: '/images/projects/2017/tete2017_whatcanhelp.png'
  imgprop: 'frame'
  selected: true
  has_equal_contribution: true
  authors:
    - name: J. Mao
      url:  'http://vccy.xyz/'
      equal_contribution: true
    - name: <strong>T. Xiao</strong>
      url:  ''
      equal_contribution: true
    - name: Y. Jiang
      url:  'https://yuningjiang.github.io/'
    - name: Z. Cao
      url:  ''
  publisher:  'CVPR 2017'
  place:    'in Honolulu, Hawaii'
  desc: 'Aggregating extra features has been considered as an effective approach to boost traditional pedestrian detection methods. The first contribution of this paper is exploring this in CNN-based pedestrian detection frameworks by evaluating the effects of different kinds of extra features quantitatively. Moreover, we propose a novel network architecture to jointly learn pedestrian detection as well as the given extra feature. By multi-task training, HyperLearner is able to utilize the information of given features and improve detection performance without extra inputs in inference.'
  tags:
    - name: 'arXiv'
      url:  'https://arxiv.org/abs/1705.02757'
    - name: 'pdf'
      url:  '/pdf/tete2017_whatcanhelp.pdf'
