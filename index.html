<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tete Xiao</title>
  
  <meta name="author" content="Tete Xiao">
  <meta name="descripction" content="Tete Xiao is the Co-Founder, CEO at Prompt AI, Inc.">
  <meta name="keywords" content="Tete Xiao, Co-Founder, CEO, Prompt AI, UC Berkeley">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">

  <script src="scripts/functions.js"></script>
</head>

<body data-gr-c-s-loaded="true">
<table width="840" border="0" align="center" cellspacing="0" cellpadding="20"><tbody><tr><td>

<!-- title -->

<p align="center">
    <pageheading>Tete Xiao</pageheading><br>
</p>

<!-- avatar and bio -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody><tr>
    <td width="32%" valign="center">
        <img src="images/tetexiao.jpg" width="100%" style="border-radius:20px">
        <p align="center">
        | <a href="https://scholar.google.com/citations?hl=en&user=dUWUGcEAAAAJ">Google Scholar</a> | <a href="https://github.com/Tete-Xiao">Github</a> |
        <br> | <a href="https://www.linkedin.com/in/tete-xiao-ba2103120/"> Linkedin</a> | <a href="pdfs/resume.pdf">CV</a> | <a href="https://twitter.com/tetexiao_ai">Twitter</a> |
        </p>
    </td>

    <td width="68%" valign="center" align="justify">
        <p> I co-founded <a href='https://promptai.co/'>Prompt AI</a> and am currently serving as its CEO in the San Francisco Bay Area.
            See more information from our <a href='https://www.prnewswire.com/news-releases/prompt-ai-raises-5-million-in-seed-funding-to-revolutionize-computer-vision-301942026.html'>press release</a>.</p>
        <p> I received a Ph.D. degree from <strong>UC Berkeley</strong>, advised by <a href='https://people.eecs.berkeley.edu/~trevor/'>Prof. Trevor Darrell</a>. 
            My research interests lie in the fields of <strong>computer vision</strong>, <strong>robotics</strong> and <strong>machine learning</strong>, with a focus on learning scalable representations via deep learning. 
            I was also affiliated with Facebook AI Research (<strong>FAIR</strong>), 
            where I was fortunate to work with <a href='https://pdollar.github.io/'>Piotr Doll√°r</a> and <a href='https://www.rossgirshick.info/'>Ross Girshick</a>.
        </p>
        <p>
            Prior to UC Berkeley, I received a B.S. degree in Intelligence Science, summa cum laude, from <a href='https://en.wikipedia.org/wiki/Peking_University' <strong>Peking University (PKU)</strong> </a> in 2019.
        </p>
    </td>
</tr></tbody></table>

<!-- news -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tbody><tr><td>
        <sectionheading>&nbsp;&nbsp;News</sectionheading>
            <ul>
                <li> <b>[Oct. 2023]</b> The <a href='https://www.prnewswire.com/news-releases/prompt-ai-raises-5-million-in-seed-funding-to-revolutionize-computer-vision-301942026.html'>press release</a> about Prompt AI. </li>
                <li> <b>[Oct. 2023]</b> Prompt AI is mentioned in <a href='https://www.wired.com/story/chatgpt-can-now-talk-to-you-and-look-into-your-life/'>an article</a> in WIRED Magazine. </li>
                <li> <b>[Apr. 2023]</b> Segment Anything (SAM) won the 2023 ICCV Best Paper Honorable Mention Award.</li>
                <li> <b>[Apr. 2023]</b> We released our latest project in vision: <a href='https://segment-anything.com/'> Segment Anything</a>, one of very few computer vision systems that <a href='https://segment-anything.com/demo'>just works in the real world</a>.</li>
                <li> <b>[Mar. 2023]</b> We released our latest project in robotics: <a href='https://humanoid-transformer.github.io/'> Learning Humanoid Locomotion with Transformers</a>, the first humanoid controlled by end-to-end neural networks. </li>
                <li> <b>[Oct. 2022]</b> We released the project: <a href='https://tetexiao.com/projects/real-mvp'> Real-World Robot Learning with Masked Visual Pre-training</a> </li>
                <li> <b>[Mar. 2022]</b> We released the <a href='https://tetexiao.com/projects/mvp'>Masked Visual Pre-training for Motor Control (MVP) </a></li>
                <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
                <div id="old_news" style="display: none;">
                <li> <b>[Oct. 2021]</b> One paper accepted to NeurIPS21! </li>
                <li> <b>[July 2021]</b> Two papers accepted to ICCV21! </li>
                <li> <b>[Jan. 2021]</b> Two papers (including one selected for oral presentation) accepted to ICLR21! </li>
                <li> <b>[Mar. 2020]</b> Our work on compositional action recognition is accepted to <a href='http://cvpr2020.thecvf.com/'>CVPR20</a>! Check out the <a href='https://joaanna.github.io/something_else/'>project page</a> with the new dataset "Something-else"! </li> 
                <li> <b>[July 2019]</b> One paper on explainable human-object interaction is accepted to <a href='http://iccv2019.thecvf.com/'>ICCV19</a>! </li>
                <li> <b>[May 2019]</b> I will join the wonderful Berkeley Artificial Intelligence Research (BAIR) Lab as a Ph.D. student at the lovely UC Berkeley in August 2019. <strong>Go Bears!</strong> </li>
                <li> <b>[Nov. 2018]</b> <a href='http://sceneparsing.csail.mit.edu/'>ADE20K</a> accepted to IJCV! We included a variety of interesting applications plus the study of synchronized batch norm for semantic segmentation. Check out the <a href='https://arxiv.org/pdf/1608.05442.pdf'>paper</a> for more details! </li>
                <li> <b>[July 2018]</b> Two papers (including one oral) accepted to <a href='https://eccv2018.org/'>ECCV18</a>! (Code is released!) </li>
                <li> <b>[May 2018]</b> Our work on contrastive samples in vision and language learning accepted to <a href='https://coling2018.org/'>COLING18</a>! (<a href='https://arxiv.org/abs/1806.10348'>Paper</a> and <a href='https://github.com/ExplorerFreda/VSE-C'>codes</a> are released.) </li>
                <li> <b>[Apr. 2018]</b> A PyTorch implementation of scene parsing networks trained on ADE20K with SOTA performance is released in conjunction with MIT CSAIL. Check out our <a href='https://github.com/CSAILVision/semantic-segmentation-pytorch'>code</a>, it's popular! </li>
                <li> <b>[Feb. 2018]</b> Two papers accepted to <a href='http://cvpr2018.thecvf.com/'>CVPR18</a>! </li>
                <li> <b>[Oct. 2017]</b> As a team member of Megvii (Face++), we won the premier challenge of object detection - <a href='https://places-coco2017.github.io/'>COCO and Places Challenges 2017</a>: the 1st places of <a href='http://cocodataset.org/#detections-leaderboard'>COCO Detection</a>, <a href='http://cocodataset.org/#keypoints-leaderboard'>COCO Keypoint</a> and <a href='http://placeschallenge.csail.mit.edu/results_challenge.html'>Places Instance Segmentation</a>, as well as the 2nd place of <a href='http://cocodataset.org/#detections-leaderboard'>COCO Instance Segmentation</a>. I was invited to present at COCO & Places Joint Workshop at <a href='http://iccv2017.thecvf.com'>ICCV17</a> in Venice, Italy. </li>
                <div>
            </div></div></ul>
    </td></tr></tbody>
</table>

<!-- publication -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr></tbody>
</table>

<!-- radosavovic2023hut -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
    <tr onmouseout="hut_stop()" onmouseover="hut_start()">

        <td width="33%" valign="center" align="center">
            <div id="hut_input" class="hidden" style="display: inline;">
                <img src="images/hut_before.png" width="100%">
            </div>
            <div id="hut_animate" style="display: none;">
                <a href="images/hut_before.png">
                    <img src="images/hut_before.png" width="100%">
                </a>
            </div>
            <script charset="utf-8" type="text/javascript">
                function hut_start() {
                    document.getElementById("hut_animate").style.display = "inline";
                    document.getElementById("hut_input").style.display = "none";
                }
    
                function hut_stop() {
                    document.getElementById("hut_animate").style.display = "none";
                    document.getElementById("hut_input").style.display = "inline";
                }
                mvp_stop() 
            </script>
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="https://humanoid-transformer.github.io/" id="radosavovic2023hut">
                    <heading>Learning Humanoid Locomotion with Transformers</heading>
                </a><br>
                <a href='https://people.eecs.berkeley.edu/~ilija/'>Ilija Radosavovic*</a>,
                <b>Tete Xiao*</b>,
                <a href='https://bikezhang106.github.io/'>Bike Zhang*</a>,
                <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell<sup>&#8224</sup></a>,<br>
                <a href='https://people.eecs.berkeley.edu/~malik/'>Jitendra Malik<sup>&#8224</sup></a>,
                <a href='https://hybrid-robotics.berkeley.edu/koushil/'>Koushil Sreenath<sup>&#8224</sup></a><br> 
                arXiv, 2023<br>
                *, <sup>&#8224</sup>: equal contribution, alphabetical order<br>
              | <a href="https://arxiv.org/abs/2303.03381">arXiv</a> |
                <a href="https://humanoid-transformer.github.io/">project page</a> |
            </p>
            <p>
                We present a sim-to-real learning-based approach for real-world humanoid locomotion.
                To the best of our knowledge, this is the first demonstration of a fully learning-based method for real-world full-sized humanoid locomotion.
            </p>
        </td>

    </tr>
    </tbody></table>

<!-- radosavovic2022realmvp -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
    <tr onmouseout="realmvp_stop()" onmouseover="realmvp_start()">

        <td width="33%" valign="center" align="center">
            <div id="realmvp_input" class="hidden" style="display: inline;">
                <img src="projects/real-mvp/teaser.png" width="100%">
            </div>
            <div id="realmvp_animate" style="display: none;">
                <a href="projects/real-mvp/teaser.png">
                    <img src="projects/real-mvp/teaser.png" width="100%">
                </a>
            </div>
            <script charset="utf-8" type="text/javascript">
                function realmvp_start() {
                    document.getElementById("realmvp_animate").style.display = "inline";
                    document.getElementById("realmvp_input").style.display = "none";
                }
    
                function realmvp_stop() {
                    document.getElementById("realmvp_animate").style.display = "none";
                    document.getElementById("realmvp_input").style.display = "inline";
                }
                mvp_stop() 
            </script>
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="https://tetexiao.com/projects/real-mvp" id="radosavovic2022realmvp">
                    <heading>Real-World Robot Learning with Masked Visual Pre-training</heading>
                </a><br>
                <a href='https://people.eecs.berkeley.edu/~ilija/'>Ilija Radosavovic*</a>,
                <b>Tete Xiao*</b>,
                <a href='https://stepjam.github.io/'>Stephen James</a>,
                <a href='https://i3.cs.berkeley.edu/'>Pieter Abbeel</a>,<br>
                <a href='https://people.eecs.berkeley.edu/~malik/'>Jitendra Malik<sup>&#8224</sup></a>,
                <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell<sup>&#8224</sup></a><br> 
                Conference on Robot Learning (CoRL), 2022<br>
                <b>Oral presentation</b><br>
                *, <sup>&#8224</sup>: equal contribution<br>
              | <a href="https://arxiv.org/abs/2210.03109">arXiv</a> |
                <a href="https://tetexiao.com/projects/real-mvp">project page</a> |
                <a href="https://github.com/ir413/mvp">code</a> |
            </p>
            <p>
                We explore self-supervised visual pre-training on images from diverse, in-the-wild videos for real-world robotic tasks.
                We train a big vision transformer on a massive collection of images from the Internet and egocentric videos, and demonstrate clearly the benefits of scaling visual pre-training for robot learning.
            </p>
        </td>

    </tr>
    </tbody></table>

<!-- xiao2022mvp -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
    <tr onmouseout="mvp_stop()" onmouseover="mvp_start()">

        <td width="33%" valign="center" align="center">
            <div id="mvp_input" class="hidden" style="display: inline;">
                <img src="images/mvp_before.png" width="100%">
            </div>
            <div id="mvp_animate" style="display: none;">
                <a href="images/mvp_after.gif">
                    <img src="images/mvp_after.gif" width="100%">
                </a>
            </div>
            <script charset="utf-8" type="text/javascript">
                function mvp_start() {
                    document.getElementById("mvp_animate").style.display = "inline";
                    document.getElementById("mvp_input").style.display = "none";
                }
    
                function mvp_stop() {
                    document.getElementById("mvp_animate").style.display = "none";
                    document.getElementById("mvp_input").style.display = "inline";
                }
                mvp_stop() 
            </script>
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="https://arxiv.org/abs/2203.06173" id="xiao2022mvp">
                    <heading>Masked Visual Pre-training for Motor Control</heading>
                </a><br>
                <b>Tete Xiao*</b>, 
                <a href='https://people.eecs.berkeley.edu/~ilija/'>Ilija Radosavovic*</a>, 
                <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell<sup>&#8224</sup></a>, 
                <a href='https://people.eecs.berkeley.edu/~malik/'>Jitendra Malik<sup>&#8224</sup></a><br> 
                Tech report, 2022<br>
                *, <sup>&#8224</sup>: equal contribution<br>
              | <a href="https://arxiv.org/abs/2203.06173">arXiv</a> |
                <a href="https://tetexiao.com/projects/mvp">project page</a> |
                <a href="https://github.com/ir413/mvp">code</a> |
            </p>
            <p>
                We show that self-supervised visual pre-training from real-world images is effective for learning motor control tasks from pixels.
            </p>
        </td>

    </tr>
    </tbody></table>

<!-- xiao2021early -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr>
    <td width="33%" valign="center" align="center">
        <img src="images/vitconv.png" style="display: inline" width="100%">
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2106.14881" id="xiao2021early">
                <heading>Early Convolutions Help Transformers See Better</heading>
            </a><br>
            <b>Tete Xiao</b>, 
            <a href='https://scholar.google.com/citations?user=QOO8OCcAAAAJ&hl=en'>Mannat Singh</a>, 
            <a href='https://scholar.google.ca/citations?user=T94KevkAAAAJ&hl=en'>Eric Mintun</a>, 
            <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell</a>, 
            <a href='https://pdollar.github.io/'>Piotr Doll√°r*</a>,<br>
            <a href='https://www.rossgirshick.info/'>Ross Girshick*</a><br> 
            Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS), 2021 <br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/abs/2106.14881">arXiv</a> |
        </p>
        <p>
            We analyze the substandard optimization behavior of ViT and propose a simple fix that dramatically increases optimization stability and also improves peak performance.
        </p>
    </td>

</tr>
</tbody></table>

<!-- xiao2021region -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="resim_stop()" onmouseover="resim_start()">
    <td width="33%" valign="center" align="center">
        <div class="image_mouseout">
            <div class="image_mouseover" id="resim_image" style="opacity: 0;">
                <img src="images/resim_after.png" ,="" width="100%">
            </div>
            <div class="image_constant" id="resim_constant" style="opacity: 1;">
                <img src="images/resim_before.png" ,="" width="100%">
            </div>
        </div>
        <script type="text/javascript">
            function resim_start() {
                document.getElementById('resim_constant').style.opacity = "0";
                document.getElementById('resim_image').style.opacity = "1";
            }

            function resim_stop() {
                document.getElementById('resim_image').style.opacity = "0";
                document.getElementById('resim_constant').style.opacity = "1";
            }
            resim_stop()
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2103.12902" id="xiao2021region">
                <heading>Region Similarity Representation Learning</heading>
            </a><br>
            <b>Tete Xiao*</b>, 
            <a href='http://people.eecs.berkeley.edu/~cjrd/'>Colorado Reed*</a>, 
            <a href='https://xiaolonw.github.io/'>Xiaolong Wang</a>, 
            <a href='https://people.eecs.berkeley.edu/~keutzer/'>Kurt Keutzer</a>, 
            <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell</a><br> 
            International Conference on Computer Vision (<b>ICCV</b>), 2021<br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/abs/2103.12902">arXiv</a> |
            <a href="https://github.com/Tete-Xiao/ReSim">code</a> |
        </p>
        <p>
            An approach to self-supervised representation learning for localization-based tasks such as object detection and segmentation.
        </p>
    </td>

</tr>
</tbody></table>

<!-- xiao2020should -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="ssl_stop()" onmouseover="ssl_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="ssl_input" class="hidden" style="display: inline;">
            <img src="images/ssl_before.png" width="100%">
        </div>
        <div id="ssl_animate" style="display: none;">
            <a href="images/ssl_after.gif">
                <img src="images/ssl_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function ssl_start() {
                document.getElementById("ssl_animate").style.display = "inline";
                document.getElementById("ssl_input").style.display = "none";
            }

            function ssl_stop() {
                document.getElementById("ssl_animate").style.display = "none";
                document.getElementById("ssl_input").style.display = "inline";
            }
            ssl_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2008.05659" id="xiao2020should">
                <heading>What Should Not Be Contrastive in Contrastive Learning</heading>
            </a><br>
            <b>Tete Xiao</b>, 
            <a href='https://xiaolonw.github.io/'>Xiaolong Wang</a>, 
            <a href='http://people.eecs.berkeley.edu/~efros/'>Alexei A. Efros</a>, 
            <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell</a><br> 
            International Conference on Learning Representations (<b>ICLR</b>), 2021<br>
          | <a href="https://arxiv.org/abs/2008.05659">arXiv</a> |
            <a href="https://www.youtube.com/watch?v=RajmUQKLG2U">video</a> |
        </p>
        <p>
            To contrast, or not to contrast, that is the question.
        </p>
    </td>

</tr>
</tbody></table>

<!-- zhang2020learning -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="cycledynamics_stop()" onmouseover="cycledynamics_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="cycledynamics_input" class="hidden" style="display: inline;">
            <img src="images/cycledynamics_before.png" width="100%">
        </div>
        <div id="cycledynamics_animate" style="display: none;">
            <a href="images/cycledynamics_after.gif">
                <img src="images/cycledynamics_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function cycledynamics_start() {
                document.getElementById("cycledynamics_animate").style.display = "inline";
                document.getElementById("cycledynamics_input").style.display = "none";
            }

            function cycledynamics_stop() {
                document.getElementById("cycledynamics_animate").style.display = "none";
                document.getElementById("cycledynamics_input").style.display = "inline";
            }
            cycledynamics_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://sjtuzq.github.io/cycle_dynamics.html" id="zhang2020learning">
                <heading>Learning Cross-domain Correspondence for Control with Dynamics Cycle-consistency</heading>
            </a><br>
            <a href='https://scholar.google.com/citations?user=mapNJjcAAAAJ&hl=en'>Qiang Zhang</a>, 
            <b>Tete Xiao</b>, 
            <a href='http://people.eecs.berkeley.edu/~efros/'>Alexei A. Efros</a>, 
            <a href='https://cs.nyu.edu/~lp91/'>Lerrel Pinto</a>, 
            <a href='https://xiaolonw.github.io/'>Xiaolong Wang</a><br> 
            International Conference on Learning Representations (<b>ICLR</b>), 2021<br>
            <b>Oral presentation</b><br>
          | <a href="https://sjtuzq.github.io/cycle_dynamics.html">project page</a> |
            <a href="https://arxiv.org/abs/2012.09811">arXiv</a> |
            <a href="https://www.youtube.com/watch?v=CZAsUM96oVg">video</a> |
        </p>
        <p>
            Learning correspondence across domains differing in representation (vision vs. internal state), physics parameters, and morphology.
        </p>
    </td>

</tr>
</tbody></table>

<!-- materzynska2020something -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="sthelse_stop()" onmouseover="sthelse_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="sthelse_input" class="hidden" style="display: inline;">
            <img src="images/sthelse_before.png" width="100%">
        </div>
        <div id="sthelse_animate" style="display: none;">
            <a href="images/sthelse_after.gif">
                <img src="images/sthelse_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function sthelse_start() {
                document.getElementById("sthelse_animate").style.display = "inline";
                document.getElementById("sthelse_input").style.display = "none";
            }

            function sthelse_stop() {
                document.getElementById("sthelse_animate").style.display = "none";
                document.getElementById("sthelse_input").style.display = "inline";
            }
            sthelse_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://joaanna.github.io/something_else/" id="materzynska2020something">
                <heading>Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks</heading>
            </a><br>
            <a href='https://joaanna.github.io/'>Joanna Materzynska</a>, 
            <b>Tete Xiao</b>, 
            <a href='https://roeiherz.github.io/'>Roei Herzig</a>, 
            <a href='https://cs-people.bu.edu/hxu/'>Huijuan Xu<sup>&#8224</sup></a>, 
            <a href='https://xiaolonw.github.io/'>Xiaolong Wang<sup>&#8224</sup></a>, 
            <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell<sup>&#8224</sup></a><br> 
            <sup>&#8224</sup>: equal advising<br>
            Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020<br>
          | <a href="https://joaanna.github.io/something_else/">project page</a> |
            <a href="https://arxiv.org/abs/1912.09930">arXiv</a> |
            <a href="https://github.com/joaanna/something_else">dataset</a> |
        </p>
        <p>
            Using Spatial-Temporal Interaction Networks (STIN) for compositional action recognition plus a new annotated dataset Something-else.
        </p>
    </td>

</tr>
</tbody></table>

<!-- xiao2019reasoning -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="dualattention_stop()" onmouseover="dualattention_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="dualattention_input" class="hidden" style="display: inline;">
            <img src="images/dualattention_before.png" width="100%">
        </div>
        <div id="dualattention_animate" style="display: none;">
            <a href="images/dualattention_after.gif">
                <img src="images/dualattention_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function dualattention_start() {
                document.getElementById("dualattention_animate").style.display = "inline";
                document.getElementById("dualattention_input").style.display = "none";
            }

            function dualattention_stop() {
                document.getElementById("dualattention_animate").style.display = "none";
                document.getElementById("dualattention_input").style.display = "inline";
            }
            dualattention_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://dual-attention-network.github.io/" id="xiao2019reasoning">
                <heading>Reasoning About Human-Object Interactions Through Dual Attention Networks</heading>
            </a><br>
            <b>Tete Xiao</b>, 
            <a href='https://researcher.watson.ibm.com/researcher/view.php?person=us-qfan'>Quanfu Fan</a>, 
            <a href='https://researcher.watson.ibm.com/researcher/view.php?person=us-dgutfre'>Dan Gutfreund</a>, 
            <a href='http://people.csail.mit.edu/mmonfort/'>Mathew Monfort</a>, 
            <a href='http://olivalab.mit.edu/audeoliva.html'>Aude Oliva</a>,
            <a href='https://boleizhou.github.io/'>Bolei Zhou</a><br> 
            International Conference on Computer Vision (<b>ICCV</b>), 2019<br>
          | <a href="https://dual-attention-network.github.io/">project page</a> |
            <a href="https://arxiv.org/pdf/1909.04743.pdf">arXiv</a> |
        </p>
        <p>
            Dual Attention Network model reasoning about human-object interactions.
        </p>
    </td>

</tr>
</tbody></table>

<!-- zhou2019semantic -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="ade20k_stop()" onmouseover="ade20k_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="ade20k_input" class="hidden" style="display: inline;">
            <img src="images/ade20k_before.jpg" width="100%">
        </div>
        <div id="ade20k_animate" style="display: none;">
            <a href="images/ade20k_after.gif">
                <img src="images/ade20k_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function ade20k_start() {
                document.getElementById("ade20k_animate").style.display = "inline";
                document.getElementById("ade20k_input").style.display = "none";
            }

            function ade20k_stop() {
                document.getElementById("ade20k_animate").style.display = "none";
                document.getElementById("ade20k_input").style.display = "inline";
            }
            ade20k_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://link.springer.com/article/10.1007/s11263-018-1140-0" id="zhou2019semantic">
                <heading>Semantic Understanding of Scenes through the ADE20K Dataset</heading>
            </a><br>
            <a href='https://boleizhou.github.io/'>Bolei Zhou</a>, 
            <a href='http://www.mit.edu/~hangzhao/'>Hang Zhao</a>, 
            <a href='https://people.csail.mit.edu/xavierpuig/'>Xavier Puig</a>, 
            <b>Tete Xiao</b>, 
            <a href='https://www.cs.utoronto.ca/~fidler/'>Sanja Fidler</a>, 
            <a href='https://groups.csail.mit.edu/vision/torralbalab/'>Adela Barriuso</a>, 
            <a href='https://groups.csail.mit.edu/vision/torralbalab/'>Antonio Torralba</a><br> 
            International Journal of Computer Vision (<b>IJCV</b>) 127, 302‚Äì321 (2019)<br>
          | <a href="http://sceneparsing.csail.mit.edu/">project page</a> |
            <a href="pdfs/zhou2019semantic.pdf">pdf</a> |
            <a href="https://arxiv.org/abs/1608.05442">arXiv</a> |
            <a href="https://github.com/CSAILVision/semantic-segmentation-pytorch">pytorch model</a> |
            <a href="http://scenesegmentation.csail.mit.edu/">demo</a> |
        </p>
        <p>
            ADE20K dataset with comprehensive analysis and applications.
        </p>
    </td>

</tr>
</tbody></table>

<!-- xiao2018unified -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="upernet_stop()" onmouseover="upernet_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="upernet_input" class="hidden" style="display: inline;">
            <img src="images/upernet_input_raw.png" width="100%">
        </div>
        <div id="upernet_parsing" style="display: none;">
            <a href="images/upernet_parsing.gif">
                <img src="images/upernet_parsing.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function upernet_start() {
                document.getElementById("upernet_parsing").style.display = "inline";
                document.getElementById("upernet_input").style.display = "none";
            }

            function upernet_stop() {
                document.getElementById("upernet_parsing").style.display = "none";
                document.getElementById("upernet_input").style.display = "inline";
            }
            upernet_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1807.10221" id="xiao2018unified">
                <heading>Unified Perceptual Parsing for Scene Understanding</heading>
            </a><br>
            <b>Tete Xiao*</b>, 
            <a href='http://yingchengliu.com/'>Yingcheng Liu*</a>, 
            <a href='https://boleizhou.github.io/'>Bolei Zhou*</a>, 
            <a href='https://yuningjiang.github.io/'>Yuning Jiang</a>,
            <a href='http://www.jiansun.org/'>Jian Sun</a><br> 
            European Conference on Computer Vision (<b>ECCV</b>), 2018<br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/pdf/1807.10221.pdf">arXiv</a> |
            <a href="https://github.com/CSAILVision/unifiedparsing">code</a> |
        </p>
        <p>
            Pyramid-like parser UPerNet used for Unified Perceptual Parsing task to recognize as many visual concepts as possible from a given image.
        </p>
    </td>

</tr>
</tbody></table>


<!-- jiang2018acquisition -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="iounet_stop()" onmouseover="iounet_start()">
    <td width="33%" valign="center" align="center">
        <div class="image_mouseout">
            <div class="image_mouseover" id="iounet_image" style="opacity: 0;">
                <img src="images/iounet_after.jpg" ,="" width="100%">
            </div>
            <img src="images/iounet_before.jpg" ,="" width="100%">
        </div>
        <script type="text/javascript">
            function iounet_start() {
                document.getElementById('iounet_image').style.opacity = "1";
            }

            function iounet_stop() {
                document.getElementById('iounet_image').style.opacity = "0";
            }
            iounet_stop()
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1807.11590" id="jiang2018acquisition">
                <heading>Acquisition of Localization Confidence for Accurate Object Detection</heading>
            </a><br>
            <a href='https://dblp.org/pid/224/0327.html'>Borui Jiang*</a>, 
            <a href='https://luoruixuan.github.io/'>Ruixuan Luo*</a>, 
            <a href='https://jiayuanm.com/'>Jiayuan Mao*</a>, 
            <b>Tete Xiao</b>, 
            <a href='https://yuningjiang.github.io/'>Yuning Jiang</a><br> 
            European Conference on Computer Vision (<b>ECCV</b>), 2018<br>
            <b>Oral presentation</b><br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/pdf/1807.11590.pdf">arXiv</a> |
            <a href="https://github.com/vacancy/PreciseRoIPooling">code</a> |
        </p>
        <p>
            Dissecting object localization through IouNet and Precise RoI Pooling.
        </p>
    </td>

</tr>
</tbody></table>

<!-- shi2018learning -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="vlcontrastive_stop()" onmouseover="vlcontrastive_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="vlcontrastive_input" class="hidden" style="display: inline;">
            <img src="images/vlcontrastive_before.png" width="100%">
        </div>
        <div id="vlcontrastive_animate" style="display: none;">
            <a href="images/vlcontrastive_after.gif">
                <img src="images/vlcontrastive_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function vlcontrastive_start() {
                document.getElementById("vlcontrastive_animate").style.display = "inline";
                document.getElementById("vlcontrastive_input").style.display = "none";
            }

            function vlcontrastive_stop() {
                document.getElementById("vlcontrastive_animate").style.display = "none";
                document.getElementById("vlcontrastive_input").style.display = "inline";
            }
            vlcontrastive_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1806.10348" id="shi2018learning">
                <heading>Learning Visually-grounded Semantics from Contrastive Adversarial Samples</heading>
            </a><br>
            <a href='https://ttic.uchicago.edu/~freda/'>Haoyue Shi*</a>,
            <a href='https://jiayuanm.com/'>Jiayuan Mao*</a>, 
            <b>Tete Xiao*</b>, 
            <a href='https://yuningjiang.github.io/'>Yuning Jiang</a>,
            <a href='http://www.jiansun.org/'>Jian Sun</a><br> 
            International Conference on Computational Linguistics (<b>COLING</b>), 2018<br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/abs/1806.10348">arXiv</a> |
            <a href="https://github.com/ExplorerFreda/VSE-C">code</a> |
        </p>
        <p>
            Constructing constrastive image-caption pairs for learning visually-grounded semantics.
        </p>
    </td>

</tr>
</tbody></table>

<!-- peng2018megdet -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr>
    <td width="33%" valign="center" align="center">
        <img src="images/megdet.png" style="display: inline" width="100%">
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1711.07240" id="peng2018megdet">
                <heading>MegDet: A Large Mini-Batch Object Detector</heading>
            </a><br>
            Chao Peng*, 
            <b>Tete Xiao*</b>, 
            Zeming Li*, 
            Yuning Jiang, 
            Xiangyu Zhang, 
            Kai Jia, 
            Gang Yu, 
            Jian Sun<br> 
            Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018 (Spotlight)<br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/abs/1711.07240">arXiv</a> |
        </p>
        <p>
            Scaling-up training of object detectors; winner of MSCOCO Challenge 2017.
        </p>
    </td>

</tr>
</tbody></table>

<!-- wang2018repulsion -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr>
    <td width="33%" valign="center" align="center">
        <img src="images/reploss.png" style="display: inline" width="100%">
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1711.07752" id="wang2018repulsion">
                <heading>Repulsion Loss: Detecting Pedestrians in a Crowd</heading>
            </a><br>
            Xinlong Wang, 
            <b>Tete Xiao</b>, 
            Yuning Jiang, 
            Shuai Shao, 
            Jian Sun, 
            Chunhua Shen<br> 
            Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018<br>
          | <a href="https://arxiv.org/abs/1711.07752">arXiv</a> |
        </p>
        <p>
            The pedestrian detector that works better for crowd occlusion.
        </p>
    </td>

</tr>
</tbody></table>

<!-- mao2017can -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr>
    <td width="33%" valign="center" align="center">
        <img src="images/whatcanhelppd.png" style="display: inline" width="100%">
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1705.02757" id="mao2017can">
                <heading>What Can Help Pedestrian Detection?</heading>
            </a><br>
            Jiayuan Mao*, 
            <b>Tete Xiao*</b>, 
            Yuning Jiang, 
            Zhimin Cao<br> 
            Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017<br>
          | <a href="https://arxiv.org/abs/1705.02757">arXiv</a> |
        </p>
    </td>

</tr>
</tbody></table>

<!-- award -->

<table width="100%" align="center" border="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Awards</sectionheading>
    <ul>
        <li><b>MSCOCO Challenge</b>, 2017</li>
        <li><b>Snap Research Scholarship</b>, 2019</li>
        <li><b>China National Scholarship</b>, Peking Univsity</li>
        <li><b>Scholarship for the Outstanding Talented</b>, Peking Univsity</li>
        <li><b>Schlumberger Scholarship</b>, Peking Univsity</li>
        <li><b>Founder Group Scholarship</b>, Peking Univsity</li>
        <li><b>Gold Medals</b>, ACM International Collegiate Programming Contest (ACM-ICPC) Asia Regional, 2016 & 2017</li>
        <li><b>Bronze Medal</b>, National Olympiad in Informatics (NOI), 2014</li>
        <li><b>Champion</b>, Shandong Province Team Selection Contest for NOI, 2014</li>
    </ul>
  </td></tr></tbody>
</table>

<!-- service -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Service</sectionheading>
  <tr>
    <td width="15%" valign="center" align="center"><img style="display: inline" width="100%" src="images/pku_seal.png"></td>
    <td width="85%" valign="center">
      Teaching Faculty, Practice in Programming (17-18 spring)
      <br><br>
      Teaching Faculty, Artificial Intelligence and Computer Vision (18-19 spring)</a>
    </td>
  </tr>
</tbody></table>

<!-- contact -->

<table width="100%" align="center" border="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Contact</sectionheading>
        <p style="margin-left: 5%;">
        Berkeley Artificial Intelligence Research Lab<br> 
        Berkeley Way West, 2121 Berkeley Way<br>
        Berkeley, CA 94704
        </p>
  </td></tr></tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody><tr><td><br>
    <p align="right"><font size="2">
        <b>Website design</b>: <a href="http://www.cs.berkeley.edu/~barron/">‚ú©</a> <a href="https://people.eecs.berkeley.edu/~pathak/">‚ú©</a><br>
        <b>Avatar photo</b>: taken in Jerusalem in July 2019 by my good friend <a href="http://yingchengliu.com/">Yingcheng Liu</a>.
    </font></p>
</td></tr></tbody></table>
        
</td></tr></tbody></table>

</body></html>
